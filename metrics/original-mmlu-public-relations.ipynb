{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q -U deepeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:14:59.936354Z","iopub.execute_input":"2025-01-08T10:14:59.936654Z","iopub.status.idle":"2025-01-08T10:15:18.332003Z","shell.execute_reply.started":"2025-01-08T10:14:59.936623Z","shell.execute_reply":"2025-01-08T10:15:18.331071Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m576.5/576.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndistributed 2024.8.0 requires dask==2024.8.0, but you have dask 2024.12.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install --no-deps xformers trl peft accelerate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:15:18.332979Z","iopub.execute_input":"2025-01-08T10:15:18.333321Z","iopub.status.idle":"2025-01-08T10:15:23.556022Z","shell.execute_reply.started":"2025-01-08T10:15:18.333288Z","shell.execute_reply":"2025-01-08T10:15:23.555091Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install -q -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:15:23.556984Z","iopub.execute_input":"2025-01-08T10:15:23.557345Z","iopub.status.idle":"2025-01-08T10:15:27.107040Z","shell.execute_reply.started":"2025-01-08T10:15:23.557314Z","shell.execute_reply":"2025-01-08T10:15:27.105856Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install -q --upgrade peft trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:15:27.109057Z","iopub.execute_input":"2025-01-08T10:15:27.109393Z","iopub.status.idle":"2025-01-08T10:15:42.258052Z","shell.execute_reply.started":"2025-01-08T10:15:27.109370Z","shell.execute_reply":"2025-01-08T10:15:42.257229Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install -q lm-format-enforcer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:15:42.259297Z","iopub.execute_input":"2025-01-08T10:15:42.259555Z","iopub.status.idle":"2025-01-08T10:15:46.032810Z","shell.execute_reply.started":"2025-01-08T10:15:42.259533Z","shell.execute_reply":"2025-01-08T10:15:46.031937Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install -q --upgrade pymilvus openai requests tqdm pandas deepeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:15:46.033791Z","iopub.execute_input":"2025-01-08T10:15:46.034052Z","iopub.status.idle":"2025-01-08T10:15:59.849134Z","shell.execute_reply.started":"2025-01-08T10:15:46.034031Z","shell.execute_reply":"2025-01-08T10:15:59.847976Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ncudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.3 which is incompatible.\nibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 18.1.0 which is incompatible.\npandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nread_key = user_secrets.get_secret(\"HF_READ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:15:59.850265Z","iopub.execute_input":"2025-01-08T10:15:59.850617Z","iopub.status.idle":"2025-01-08T10:15:59.994967Z","shell.execute_reply.started":"2025-01-08T10:15:59.850586Z","shell.execute_reply":"2025-01-08T10:15:59.994252Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token= read_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:15:59.995783Z","iopub.execute_input":"2025-01-08T10:15:59.995996Z","iopub.status.idle":"2025-01-08T10:16:00.551586Z","shell.execute_reply.started":"2025-01-08T10:15:59.995978Z","shell.execute_reply":"2025-01-08T10:16:00.550655Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import json\nfrom pydantic import BaseModel\nimport torch\nfrom lmformatenforcer import JsonSchemaParser\nfrom lmformatenforcer.integrations.transformers import (\n    build_transformers_prefix_allowed_tokens_fn,\n)\nfrom transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n\nfrom deepeval.models import DeepEvalBaseLLM\n\n\nclass CustomModel(DeepEvalBaseLLM):\n    def __init__(self):\n        quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=True,\n        )\n\n        model_4bit = AutoModelForCausalLM.from_pretrained(\n            \"unsloth/gemma-2b-it-bnb-4bit\",\n            device_map=\"auto\",\n            quantization_config=quantization_config,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"unsloth/gemma-2b-it-bnb-4bit\"\n        )\n\n        self.model = model_4bit\n        self.tokenizer = tokenizer\n\n    def load_model(self):\n        return self.model\n\n    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n        model = self.load_model()\n        # Initialize the pipeline for text generation\n        gen_pipeline = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=self.tokenizer,\n            use_cache=True,\n            device_map=\"auto\",\n            max_length=2500,\n            do_sample=True,\n            top_k=5,\n            num_return_sequences=1,\n            eos_token_id=self.tokenizer.eos_token_id,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n\n        # Create parser required for JSON confinement using lmformatenforcer\n        parser = JsonSchemaParser(schema.schema())\n        prefix_function = build_transformers_prefix_allowed_tokens_fn(\n            gen_pipeline.tokenizer, parser\n        )\n\n        # Output and load valid JSON\n        output_dict = gen_pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n        output = output_dict[0][\"generated_text\"][len(prompt):]\n        json_result = json.loads(output)\n\n        # Return valid JSON object according to the schema DeepEval supplied\n        return schema(**json_result)\n\n    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n        return self.generate(prompt, schema)\n\n    def get_model_name(self):\n        return \"Gemma-2B Unsloth\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:16:00.552580Z","iopub.execute_input":"2025-01-08T10:16:00.552899Z","iopub.status.idle":"2025-01-08T10:16:14.508050Z","shell.execute_reply.started":"2025-01-08T10:16:00.552876Z","shell.execute_reply":"2025-01-08T10:16:14.507159Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from deepeval.benchmarks import MMLU\nfrom deepeval.benchmarks.tasks import MMLUTask\n\ncustom_llm = CustomModel()\n\nmm_tasks = [\n    MMLUTask.PUBLIC_RELATIONS,\n]\n\nbenchmark = MMLU(\n    tasks=mm_tasks,\n    n_shots=3\n)\n\n\nbenchmark.evaluate(model=custom_llm)\nprint(benchmark.overall_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T10:16:14.509024Z","iopub.execute_input":"2025-01-08T10:16:14.509363Z","iopub.status.idle":"2025-01-08T10:48:31.752357Z","shell.execute_reply.started":"2025-01-08T10:16:14.509323Z","shell.execute_reply":"2025-01-08T10:48:31.751427Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85f153d1db2d4b579992f08257dec0b3"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.07G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23509b91daae48109f183bd0ffe0beaa"}},"metadata":{}},{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4df21c4a92e645d28d3826b953831d59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c867cc4fa6ed43678ec02c6e4b513d93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae282a48167841a68628974ea050bc1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4122b827e2aa4c2cb9a3cf3c42b4c8e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d28bedf8a554093a8b673d47e4049e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/28.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e055c7a6eb346619231359301c71ccd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mmlu.py:   0%|          | 0.00/5.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dfe7bfdc8e04e2cbdbf34d2c85f8e4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/166M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25ea648fb2ec4ed1a218a1f59f0184b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/110 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a4797262784a51a0667cfa828e5e07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/12 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcfb59bff2b74399b171b95d315772e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33337889477b42019aeb42a5aecd95d0"}},"metadata":{}},{"name":"stderr","text":"Processing public_relations:   0%|          | 0/110 [00:00<?, ?it/s]Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nProcessing public_relations:   1%|          | 1/110 [00:18<32:59, 18.16s/it]Device set to use cuda:0\nProcessing public_relations:   2%|▏         | 2/110 [00:35<31:38, 17.58s/it]Device set to use cuda:0\nProcessing public_relations:   3%|▎         | 3/110 [00:52<30:47, 17.26s/it]Device set to use cuda:0\nProcessing public_relations:   4%|▎         | 4/110 [01:09<30:10, 17.08s/it]Device set to use cuda:0\nProcessing public_relations:   5%|▍         | 5/110 [01:25<29:36, 16.92s/it]Device set to use cuda:0\nProcessing public_relations:   5%|▌         | 6/110 [01:42<29:04, 16.77s/it]Device set to use cuda:0\nProcessing public_relations:   6%|▋         | 7/110 [01:59<29:12, 17.01s/it]Device set to use cuda:0\nProcessing public_relations:   7%|▋         | 8/110 [02:16<29:03, 17.09s/it]Device set to use cuda:0\nProcessing public_relations:   8%|▊         | 9/110 [02:33<28:38, 17.01s/it]Device set to use cuda:0\nProcessing public_relations:   9%|▉         | 10/110 [02:50<28:17, 16.98s/it]Device set to use cuda:0\nProcessing public_relations:  10%|█         | 11/110 [03:07<28:01, 16.98s/it]Device set to use cuda:0\nProcessing public_relations:  11%|█         | 12/110 [03:24<27:52, 17.06s/it]Device set to use cuda:0\nProcessing public_relations:  12%|█▏        | 13/110 [03:41<27:09, 16.80s/it]Device set to use cuda:0\nProcessing public_relations:  13%|█▎        | 14/110 [03:58<27:08, 16.96s/it]Device set to use cuda:0\nProcessing public_relations:  14%|█▎        | 15/110 [04:15<26:55, 17.00s/it]Device set to use cuda:0\nProcessing public_relations:  15%|█▍        | 16/110 [04:32<26:44, 17.07s/it]Device set to use cuda:0\nProcessing public_relations:  15%|█▌        | 17/110 [04:49<26:18, 16.97s/it]Device set to use cuda:0\nProcessing public_relations:  16%|█▋        | 18/110 [05:06<26:09, 17.06s/it]Device set to use cuda:0\nProcessing public_relations:  17%|█▋        | 19/110 [05:23<25:43, 16.96s/it]Device set to use cuda:0\nProcessing public_relations:  18%|█▊        | 20/110 [05:40<25:28, 16.98s/it]Device set to use cuda:0\nProcessing public_relations:  19%|█▉        | 21/110 [05:57<25:12, 16.99s/it]Device set to use cuda:0\nProcessing public_relations:  20%|██        | 22/110 [06:13<24:38, 16.80s/it]Device set to use cuda:0\nProcessing public_relations:  21%|██        | 23/110 [06:31<24:45, 17.07s/it]Device set to use cuda:0\nProcessing public_relations:  22%|██▏       | 24/110 [06:48<24:29, 17.08s/it]Device set to use cuda:0\nProcessing public_relations:  23%|██▎       | 25/110 [07:04<23:48, 16.80s/it]Device set to use cuda:0\nProcessing public_relations:  24%|██▎       | 26/110 [07:21<23:36, 16.86s/it]Device set to use cuda:0\nProcessing public_relations:  25%|██▍       | 27/110 [07:38<23:08, 16.73s/it]Device set to use cuda:0\nProcessing public_relations:  25%|██▌       | 28/110 [07:55<23:01, 16.84s/it]Device set to use cuda:0\nProcessing public_relations:  26%|██▋       | 29/110 [08:12<22:56, 16.99s/it]Device set to use cuda:0\nProcessing public_relations:  27%|██▋       | 30/110 [08:29<22:29, 16.87s/it]Device set to use cuda:0\nProcessing public_relations:  28%|██▊       | 31/110 [08:45<22:08, 16.81s/it]Device set to use cuda:0\nProcessing public_relations:  29%|██▉       | 32/110 [09:03<22:04, 16.98s/it]Device set to use cuda:0\nProcessing public_relations:  30%|███       | 33/110 [09:20<21:45, 16.96s/it]Device set to use cuda:0\nProcessing public_relations:  31%|███       | 34/110 [09:37<21:26, 16.93s/it]Device set to use cuda:0\nProcessing public_relations:  32%|███▏      | 35/110 [09:53<20:58, 16.78s/it]Device set to use cuda:0\nProcessing public_relations:  33%|███▎      | 36/110 [10:10<20:55, 16.96s/it]Device set to use cuda:0\nProcessing public_relations:  34%|███▎      | 37/110 [10:27<20:37, 16.95s/it]Device set to use cuda:0\nProcessing public_relations:  35%|███▍      | 38/110 [10:44<20:06, 16.75s/it]Device set to use cuda:0\nProcessing public_relations:  35%|███▌      | 39/110 [11:02<20:22, 17.22s/it]Device set to use cuda:0\nProcessing public_relations:  36%|███▋      | 40/110 [11:18<19:45, 16.94s/it]Device set to use cuda:0\nProcessing public_relations:  37%|███▋      | 41/110 [11:36<19:36, 17.04s/it]Device set to use cuda:0\nProcessing public_relations:  38%|███▊      | 42/110 [11:53<19:24, 17.13s/it]Device set to use cuda:0\nProcessing public_relations:  39%|███▉      | 43/110 [12:10<19:03, 17.07s/it]Device set to use cuda:0\nProcessing public_relations:  40%|████      | 44/110 [12:27<18:54, 17.19s/it]Device set to use cuda:0\nProcessing public_relations:  41%|████      | 45/110 [12:45<18:46, 17.33s/it]Device set to use cuda:0\nProcessing public_relations:  42%|████▏     | 46/110 [13:02<18:20, 17.20s/it]Device set to use cuda:0\nProcessing public_relations:  43%|████▎     | 47/110 [13:19<18:04, 17.21s/it]Device set to use cuda:0\nProcessing public_relations:  44%|████▎     | 48/110 [13:36<17:37, 17.06s/it]Device set to use cuda:0\nProcessing public_relations:  45%|████▍     | 49/110 [13:53<17:26, 17.16s/it]Device set to use cuda:0\nProcessing public_relations:  45%|████▌     | 50/110 [14:10<17:11, 17.19s/it]Device set to use cuda:0\nProcessing public_relations:  46%|████▋     | 51/110 [14:27<16:41, 16.98s/it]Device set to use cuda:0\nProcessing public_relations:  47%|████▋     | 52/110 [14:44<16:30, 17.08s/it]Device set to use cuda:0\nProcessing public_relations:  48%|████▊     | 53/110 [15:01<16:04, 16.93s/it]Device set to use cuda:0\nProcessing public_relations:  49%|████▉     | 54/110 [15:18<15:54, 17.05s/it]Device set to use cuda:0\nProcessing public_relations:  50%|█████     | 55/110 [15:35<15:42, 17.13s/it]Device set to use cuda:0\nProcessing public_relations:  51%|█████     | 56/110 [15:52<15:18, 17.01s/it]Device set to use cuda:0\nProcessing public_relations:  52%|█████▏    | 57/110 [16:10<15:07, 17.13s/it]Device set to use cuda:0\nProcessing public_relations:  53%|█████▎    | 58/110 [16:26<14:44, 17.01s/it]Device set to use cuda:0\nProcessing public_relations:  54%|█████▎    | 59/110 [16:43<14:21, 16.90s/it]Device set to use cuda:0\nProcessing public_relations:  55%|█████▍    | 60/110 [17:00<14:11, 17.02s/it]Device set to use cuda:0\nProcessing public_relations:  55%|█████▌    | 61/110 [17:17<13:54, 17.03s/it]Device set to use cuda:0\nProcessing public_relations:  56%|█████▋    | 62/110 [17:34<13:37, 17.03s/it]Device set to use cuda:0\nProcessing public_relations:  57%|█████▋    | 63/110 [17:52<13:27, 17.18s/it]Device set to use cuda:0\nProcessing public_relations:  58%|█████▊    | 64/110 [18:09<13:11, 17.21s/it]Device set to use cuda:0\nProcessing public_relations:  59%|█████▉    | 65/110 [18:26<12:53, 17.19s/it]Device set to use cuda:0\nProcessing public_relations:  60%|██████    | 66/110 [18:44<12:38, 17.24s/it]Device set to use cuda:0\nProcessing public_relations:  61%|██████    | 67/110 [19:01<12:18, 17.18s/it]Device set to use cuda:0\nProcessing public_relations:  62%|██████▏   | 68/110 [19:18<12:00, 17.15s/it]Device set to use cuda:0\nProcessing public_relations:  63%|██████▎   | 69/110 [19:35<11:44, 17.18s/it]Device set to use cuda:0\nProcessing public_relations:  64%|██████▎   | 70/110 [19:52<11:28, 17.21s/it]Device set to use cuda:0\nProcessing public_relations:  65%|██████▍   | 71/110 [20:09<11:04, 17.03s/it]Device set to use cuda:0\nProcessing public_relations:  65%|██████▌   | 72/110 [20:26<10:49, 17.10s/it]Device set to use cuda:0\nProcessing public_relations:  66%|██████▋   | 73/110 [20:43<10:31, 17.08s/it]Device set to use cuda:0\nProcessing public_relations:  67%|██████▋   | 74/110 [21:00<10:11, 16.98s/it]Device set to use cuda:0\nProcessing public_relations:  68%|██████▊   | 75/110 [21:17<09:55, 17.00s/it]Device set to use cuda:0\nProcessing public_relations:  69%|██████▉   | 76/110 [21:34<09:40, 17.08s/it]Device set to use cuda:0\nProcessing public_relations:  70%|███████   | 77/110 [21:51<09:15, 16.84s/it]Device set to use cuda:0\nProcessing public_relations:  71%|███████   | 78/110 [22:07<08:53, 16.67s/it]Device set to use cuda:0\nProcessing public_relations:  72%|███████▏  | 79/110 [22:24<08:42, 16.86s/it]Device set to use cuda:0\nProcessing public_relations:  73%|███████▎  | 80/110 [22:41<08:21, 16.72s/it]Device set to use cuda:0\nProcessing public_relations:  74%|███████▎  | 81/110 [22:58<08:13, 17.01s/it]Device set to use cuda:0\nProcessing public_relations:  75%|███████▍  | 82/110 [23:15<07:55, 16.99s/it]Device set to use cuda:0\nProcessing public_relations:  75%|███████▌  | 83/110 [23:32<07:37, 16.93s/it]Device set to use cuda:0\nProcessing public_relations:  76%|███████▋  | 84/110 [23:49<07:20, 16.95s/it]Device set to use cuda:0\nProcessing public_relations:  77%|███████▋  | 85/110 [24:05<06:59, 16.80s/it]Device set to use cuda:0\nProcessing public_relations:  78%|███████▊  | 86/110 [24:23<06:49, 17.06s/it]Device set to use cuda:0\nProcessing public_relations:  79%|███████▉  | 87/110 [24:40<06:32, 17.05s/it]Device set to use cuda:0\nProcessing public_relations:  80%|████████  | 88/110 [24:57<06:12, 16.91s/it]Device set to use cuda:0\nProcessing public_relations:  81%|████████  | 89/110 [25:14<05:57, 17.01s/it]Device set to use cuda:0\nProcessing public_relations:  82%|████████▏ | 90/110 [25:32<05:43, 17.19s/it]Device set to use cuda:0\nProcessing public_relations:  83%|████████▎ | 91/110 [25:48<05:25, 17.12s/it]Device set to use cuda:0\nProcessing public_relations:  84%|████████▎ | 92/110 [26:05<05:06, 17.05s/it]Device set to use cuda:0\nProcessing public_relations:  85%|████████▍ | 93/110 [26:22<04:49, 17.04s/it]Device set to use cuda:0\nProcessing public_relations:  85%|████████▌ | 94/110 [26:40<04:35, 17.21s/it]Device set to use cuda:0\nProcessing public_relations:  86%|████████▋ | 95/110 [26:57<04:19, 17.29s/it]Device set to use cuda:0\nProcessing public_relations:  87%|████████▋ | 96/110 [27:14<04:00, 17.18s/it]Device set to use cuda:0\nProcessing public_relations:  88%|████████▊ | 97/110 [27:31<03:41, 17.05s/it]Device set to use cuda:0\nProcessing public_relations:  89%|████████▉ | 98/110 [27:48<03:23, 16.92s/it]Device set to use cuda:0\nProcessing public_relations:  90%|█████████ | 99/110 [28:06<03:10, 17.34s/it]Device set to use cuda:0\nProcessing public_relations:  91%|█████████ | 100/110 [28:23<02:53, 17.33s/it]Device set to use cuda:0\nProcessing public_relations:  92%|█████████▏| 101/110 [28:40<02:34, 17.19s/it]Device set to use cuda:0\nProcessing public_relations:  93%|█████████▎| 102/110 [28:58<02:18, 17.31s/it]Device set to use cuda:0\nProcessing public_relations:  94%|█████████▎| 103/110 [29:14<01:59, 17.09s/it]Device set to use cuda:0\nProcessing public_relations:  95%|█████████▍| 104/110 [29:31<01:42, 17.06s/it]Device set to use cuda:0\nProcessing public_relations:  95%|█████████▌| 105/110 [29:49<01:25, 17.19s/it]Device set to use cuda:0\nProcessing public_relations:  96%|█████████▋| 106/110 [30:06<01:08, 17.12s/it]Device set to use cuda:0\nProcessing public_relations:  97%|█████████▋| 107/110 [30:22<00:50, 16.96s/it]Device set to use cuda:0\nProcessing public_relations:  98%|█████████▊| 108/110 [30:39<00:33, 16.97s/it]Device set to use cuda:0\nProcessing public_relations:  99%|█████████▉| 109/110 [30:57<00:17, 17.12s/it]Device set to use cuda:0\nProcessing public_relations: 100%|██████████| 110/110 [31:14<00:00, 17.04s/it]","output_type":"stream"},{"name":"stdout","text":"MMLU Task Accuracy (task=public_relations): 0.2545454545454545\nOverall MMLU Accuracy: 0.2545454545454545\n0.2545454545454545\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}